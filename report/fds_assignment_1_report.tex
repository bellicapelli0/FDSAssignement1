\documentclass{article}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{mathtools}
\usepackage[hidelinks]{hyperref}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{cleveref}
\usepackage{listings}
\usepackage{float}
\usepackage{appendix}
\usepackage[useregional]{datetime2}
%\usepackage{interval}
\usepackage[backend=biber, style=apa]{biblatex}
\addbibresource{cap_bibliography.bib}

\usepackage[margin=1in, left=1.5in, includefoot]{geometry}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[R]{\thepage}
\lhead{\textbf{Assignment Team 12}}

\newcommand\email[1]{
    \href{mailto:#1}{\url{#1}}
}

\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  allcolors=[rgb]{0.129,0.341,0.769}
}

\setlength{\parindent}{0em}
\begin{document}
\begin{titlepage}
  \newcommand{\HRule}{\rule{\linewidth{0.5mm}}}

    \begin{center}
      %\HRule
      %\\[0.5 cm]
      \rule{\linewidth}{0.5mm} \\
      \vspace{0.5 cm}
      \huge{\bfseries Assignment 1: Studying 2016 US Elections Through
        Analysing Twitter Data}
      \rule{\linewidth}{0.5mm}
      \\[1.2 cm]
      \begin{minipage}{0.45\textwidth}
        \begin{flushleft}
        \large
        António Mendes\\
        {\small\email{17amendes@gmail.com}} \\
        {\small 11925051}
        \end{flushleft}
      \end{minipage}
      ~
      \begin{minipage}{0.4\textwidth}
        \begin{flushright}
          \large
          Judit Győrfi\\
          {\small \email{judit.gyorfi@student.uva.nl}}\\
          {\small 13209647}
        \end{flushright}
      \end{minipage}
      \\[1cm]
      \begin{minipage}{0.45\textwidth}
        \begin{flushleft}
        \large
        Orlando Scarpa\\
        {\small \email{orlando.scarpa@student.uva.nl}}\\
        {\small 13266918}
        \end{flushleft}
      \end{minipage}
      ~
      \begin{minipage}{0.4\textwidth}
        \begin{flushright}
          \large
          Adam Horvath-Reparszky\\
          {\small \email{adam.horvath-reparszky@student.uva.nl}}\\
          {\small 13326481}
        \end{flushright}
      \end{minipage}
      \\[1cm]
      \begin{minipage}{0.5\textwidth}
        \begin{center}
          \large
          Miklos Kosarszky\\
          {\small\email{miklos.kosarszky@student.uva.nl}}\\
          {\small 13242857}
        \end{center}
      \end{minipage}
      \vfill
      \vfill
      \vfill
    \end{center}
  \end{titlepage}

  \tableofcontents
  \thispagestyle{empty}
  \pagebreak
  \setcounter{page}{1}
  \setcounter{section}{0}
  \section{Data Preparation}
  After extracting all the json objects and flattening all the fields
  the DataFrame had 35 columns, Of these, 10 were selected as valuable
  insights into the data.\\
  
  Using the columns relating to the language and country code, the
  DataFrame was reduced to only tweets in english and posted from the
  United States. After this, the column containing the full name of
  the location of origin of the tweet was used to extract the two
  letter code of the state of origin, keeping in mind to exclude
  territories whose inhabitants don’t vote for the president like
  Puerto Rico and Guam. Of the 517,724 tweets in english from the
  United States, less than 5000 tweets were excluded in this
  manner. To clean the text and prepare it to be tokenized, all text
  in the tweets was stripped of punctuation, special characters, new
  lines, hyperlinks and trailing spaces. Furthermore, stop words were
  removed and select hashtags and mentions were counted to give a
  rudimentary estimate of the candidate being talked about in each
  tweet.
  
  %\section{Data Splitting}
  \section{Sentiment Analysis}
  \subsection{Multinomial Naïve Bayes}
  Our first approach to predict the sentiment of the tweets, was to
  use the Multinomial Naive Bayes. It is generally used where there
  are discrete features ( e.g. word counts in a text classification
  problem). The multinomial distribution describes the probability of
  observing counts among several categories, this was the reason why
  we chose this first. To feed the machine learning algorithm, we
  vectorized our string data to numerical values, for this  we used
  CountVectorization and Tf-IDF transformation. Moreover we used
  pipeline technique which is a built-in function of scikit-learn to
  pre-define a workflow of algorithm. After selecting our model, we
  had to validate it with the existing training data set, so we used
  the conventional \texttt{train\_test\_split} technique to split the
  training data set with the test size of $20\%$ and let our pipeline
  model be validated on the split data sets. The model reached an
  accuracy of $75,8\%$, which is relatively low, so we decided to look
  for another, hopefully more accurate approach. 
  
  \subsection{Neural Networks}
  \begin{wrapfigure}{r}{0.30\linewidth}
    \includegraphics[trim=0 0 0 100, width=\linewidth]{{Dense.png}}
    \centering
    \captionsetup{justification=centering}
    \caption{Diagram of densely layered Neural Network}
    \vspace{-110pt}
    \label{fig:dense}
  \end{wrapfigure}
  
  To attempt to supplement the relatively low accuracy of the
  Multinomial Naïve Bayes, three different Neural Network approaches
  were tested. All Neural Networks were developed using Tensorflow 2.0
  + Keras, and were trained, validated and tested using the
  Sentiment140 dataset containing $1.6$ million labeled tweets.%\\
  
  The first approach was a simple sequence of three Dense layers,
  interleaved with dropout layers with a rate of 0.2 to avoid
  overfitting. The input tweets were first tokenized and converted
  into a matrix representing the count of how many times a given word
  appeared in a tweet. The model was trained on $80\%$ of the data,
  with the remaining $20\%$ being used as the test input. The fit
  function also used a $25\%$ validation size, to guarantee equal
  dimensions between the test and validation dataset. The model was
  fitted for $10$ epochs with a batch size of $32$, then tested on the
  test dataset, reaching a final accuracy of $84.7\%$. While this was
  considerably higher than the Naïve Bayes approach, we decided to try
  some other more sophisticated approaches to attempt to further
  optimize the results.%\\

  %\begin{wrapfigure}{r}{0.25\linewidth}
  %  \vspace{-20pt}
  %  \begin{center}
  %    \includegraphics[width=\linewidth]{{CNN.png}}
  %  \end{center}
  %  \captionsetup{justification=centering}
  %  \caption{Diagram of Convolutional Neural Network} 
  %  \label{fig:cnn}
  %  \vspace{-20pt}
  %\end{wrapfigure}

  The second approach used a 1-Dimensional Convolutional layer in the
  hope that the net would be able to recognize patterns in sequences
  of words. For this approach, tokenization was achieved with text
  sequences to maintain word order. The same $80-20$ split was used to
  separate the input data, with a $0.25$ validation split of the train
  data. Before the 1D Convolutional layer, an embedding layer was
  added to map the tokenizer vocabulary indices to embedding
  indices. The Convolutional layer was followed by two Dense
  layers. Dropout layers were appropriately inserted to avoid
  overfitting. Once again, the net was trained on the same dataset as
  the previous approach with $10$ epochs and a batch size of
  $32$. Once tested on the test dataset, the model reached an accuracy
  of $79.4\%$ that, while higher than the Naïve Bayes approach, was
  still lower than the simple dense model.%\\

  %\begin{wrapfigure}[11]{r}{0.30\linewidth}
  %  \includegraphics[trim=0 0 0 40, width=\linewidth]{{LSTM.png}}
  %  \vspace{-10pt}
  %  \centering
  %  \captionsetup{justification=centering}
  %  \caption{Diagram of LSMT Network} 
  %  \label{fig:lsmt}
  %\end{wrapfigure}
  
  The final approach was to use a Recurrent Neural Network to make use
  of the memory state of these networks and better identify patterns
  in the tweets. Long Short Term Memory (LSMT) was chosen as it’s more
  sophisticated than basic RNNs and avoids the vanishing gradient
  problem caused by these. The Train-Validation-Test split was
  identical to the previous approaches, and tokenization was achieved
  using the same method as the Convolutional Approach.%\\

  The LSTM layer was preceded by an embedding and dropout layer, and
  followed by a Dense layer (with an output dimension of $[?,2]$ since
  in this approach the sentiment target data was represented with
  dummies). The model was once again fitted on a batch size of $32$
  for $10$ epochs, achieving a final accuracy of $82.1\%$.%\\

  All three approaches reached a higher accuracy metric than the
  Multinomial Naïve Bayes approach, but to our surprise, the simplest
  model achieved the best results. After using the examined 2016 tweet
  data as the input for the predict functions of all three models and
  manually checking samples of tweets where the models disagreed on
  the sentiment, we found the Convolutional model to be far less
  accurate than the other two in predicting the sentiment of the
  tweets, and the Simple model to be very slightly more accurate than
  the recurrent one. For this reason, we decided to finally use the
  simple model’s predictions of sentiment for the twitter data.%\\
  
  \section{Topic Modelling}

  In addition to sentiment analysis, an attempt was also made to model
  topics for the tweets. The hope here was to separate the two
  political candidates into two topics. The main approach explored is
  a Latent Dirichlect Allocation model (LDA) trained on the same data
  as was used before. LDA is an unsupervised soft clustering approach,
  so it allows for some flexibility between the topics. To optimise the
  model, the $\alpha$ and $\eta$ hyperparameters are turned according
  to the gensim Python package's coherence measure. The coherence
  measure "rate[s] topics according to their understandability". It is
  bounded between 0 and 1, with 1 being the most optimal score and 0
  being the least optimal score. The table below illustrates
  descriptive statistics of the coherence value for the lda model for
  two, three, and four topics specified. 
  
  \begin{table}[H]
    \centering
    \begin{tabular}{||p{2cm} p{3cm} p{3cm} p{2.5cm} ||}
      \hline
      Topics & Mean Coherence & Median Coherence &  Max Coherence \\ \hline
      2 & 0.281548 & 0.261847 & 0.498121 \\
      3 & 0.402202 & 0.411127 & 0.496927 \\
      4 & 0.377042 & 0.386631 & 0.473121 \\
      \hline
    \end{tabular}
    \caption{\label{tab:lda_hyperparameter_table} Coherence Values Against Number of Topics}
  \end{table}

  The coherence values for modelling three and four topics are
  included to provide context to interpret the model for two
  topics. The mean coherence for two topics is substantially lower at
  $0.281548$, but it is quickly explained by the slightly lower median, so
  the distribution of coherence values across the different
  combinations of hyperparameters is skewed towards $0$. However, the
  model for two topics has the highest coherence value. In fact, from
  two to four, the maximum coherence value progressively lowers,
  suggesting that increasing the number of topics only further reduces
  the topic coherence. As such, we choose the hyperparameters for the
  model with two topics with the max coherence of $0.498121$. These
  values are $\alpha = 0.01$ and $\eta = "symmetric"$. A coherence of
  $\approx 0.500$ is still not ideal, and it possibly suggests that
  the topics might not be coherent enough.
  
  After tuning the lda model, closer inspection at the results reveals
  that indeed the topics are not very coherent. Below is a list of
  filtered tweets in the first topic:

  \begin{itemize}
    \item Words in Tweet \#114226: such shit realdonaldtrump would have
      no campaign w o the constant attention he gets from the elite
      media
    \item Words in Tweet \#33233:  realdonaldtrump you obviously have
      no idea how things work dummy
    \item Words in Tweet \#99920:  seanhannity newtgingrich
      realdonaldtrump he s cooke sean it s over trump is a loser
    \item Words in Tweet \#34744: she is terrified of people trump will
      rip her a new one at the debates
    \item Words in Tweet \#29191:  cnn why do u put on these brain dead individuals the world is watching america usually keeps these morons off tv 
  \end{itemize}

  While the first three tweets in the list can be interpreted as
  "critical of Trump", the same interpretation cannot be applied to
  the other two tweets. This selection of tweets is representative of
  a mixture of tweets in this topic. A similar problem can be found in
  the tweets in the second topic. Because of the poor performance of
  this lda model even after tuning, we fell back on using the hashtags
  found in the data preparation for the other analyses in this paper.    

  %\begin{itemize}
  %  \item Words in Tweet \#105854: thx to the hillarists i know now i m
  %    not a progressive i m a flaming misogynist bolshevik treehugger
  %    demexit neverhillary jillnothill
  %    
  %  \item Words in Tweet \#61397: if hillary doesn t press the red
  %    button she s weak and if she does she s hysterical
  %  \item  Words in Tweet \#77246:  realdonaldtrump foxnews crooked
  %    hillary dogging the media now 273 days disgraceful she s hiding
  %    medical amp drug problems absolutely
  %  \item Words in Tweet \#16154:  realdonaldtrump hillary lied to
  %    congress period does anyone believe she will go to jail martha
  %    stewart went to prison for far less wtf
  %  \item Words in Tweet \#100027:  realdonaldtrump andersoncooper
  %    donlemon come on cheating lying hillary getting the answers during the townhall 
  %\end{itemize}

  %Because of the poor performance of this lda model even after tuning,
  %we fell back on using the hashtags found in the data preparation for
  %the other analyses in this paper.   
      
  \section{Polarity}
  The already cleaned data provides us with many possibilities in
  terms of data analysis. One of the main focus of the analysis is on
  sentiment analysis of the Twitter texts. Python has a wide variety
  of Natural Language Processing (NLP) tools that are helpful when
  looking for the sentiment value of a particular text, and in our
  case, tweets. Some of the packages, such as NLTK and TextBlob can
  deal with strings mathematically. To find out the polarity of our
  tweets, we have used the TextBlob tool which classifies the text
  into negative, neutral and positive tweets between the values $-1$ and
  $1$, where negative numbers tend to refer to tweets with negative
  wording and positives refer to tweets with positive words. TextBlob
  also creates a second output called subjectivity, with values
  between $0$ and $1$  where $0$ is an objective tweet and $1$ is a subjective
  one. In this assignment, however, we are only focusing on polarity
  as this can be more correlated to actual vote results.%\\

  The first step in our analysis is to create a new column called
  “polarity” in the already cleaned data with the TextBlob package
  based on the tweet texts. The new column will contain the values of
  the polarities of the particular tweets. Next, we are grouping the
  data by states and taking the average value of the polarities per
  state. As a result ,we get the sorted polarity values for the 50
  states. We can observe that the average polarity values of the
  tweets in general are between $-0.2$ and $0.2$.

  In order to get a better insight on how tweets can be related to
  votes results, we have split up our dataset to tweets which are
  mainly about Trump and to tweets about Hillary. After that, we
  follow the same procedure as before, and group the average polarity
  scores of the two candidates per state. Next, we load an external
  dataset from the $2016$ U.S elections
  (“usa-2016-presidential-election-by-county.csv”) to compare the
  results with the polarity of the tweets. The results state that
  Trump got the most positive comments from states like Montana,
  Wisconsin and Kentucky. These are states where Trump also got more
  votes than Hillary Clinton.  
  
  \begin{table}[H]
    \centering
    \begin{tabular}{||p{2cm} p{3cm} p{3cm} ||}
      \hline
      state & polarity & name \\ \hline
      NE & 0.0819 & Nebraska \\
      MT & 0.0825 & Montana \\
      WI & 0.1080 & Wisconsin \\
      KT & 0.1120 & Kentucky \\
      \hline
    \end{tabular}
    \caption{\label{tab:} }
  \end{table}

  As for Hillary, we can conclude that she received negative tweets in
  states which are Republican bastions such as Idaho, North Dakota and
  Wyoming. Correspondingly, she received positive tweets from states
  such as Washington D.C and New Hampshire which indeed voted for
  Hillary. We have calculated a correlation score between polarity and
  the winner of a state ($1$ : Trump, $0$: Hillary). The resulting
  correlation for tweets containing Trump and winner:Trump  is
  $0.257$. The result states that the more positive the tweet about
  Trump is, the more likely that state has voted for Trump. Vice
  verse, if the tweets are more positive about Hillary, it is less
  likely that people in that state voted for Trump. (Correlation:
  $-0.189$). However, these correlations are not high enough (smaller
  than $0.3$) to say that the polarity of a tweet has a decisive impact
  on the vote results.

  \begin{table}[H]
    \centering
    \begin{tabular}{||p{2cm} p{3cm} p{3cm} ||}
      \hline
      state & polarity & name \\ \hline
      ID & -0.0236 & Idaho \\
      ND & -0.0158 & North Dakota \\
      WY & -0.0148 & Wyoming \\
      UT & -0.0133 & Utah \\
      DC & 0.0786 & District of Columbia \\
      NH & 0.0822 & New Hampshire\\
      \hline
    \end{tabular}
    \caption{\label{tab:} }
  \end{table}

  To get a better overview, we have included a few maps which reflect
  whether the tweets in a state were positive or negative about
  Hillary or Trump. Furthermore, we have also added a map about the
  votes results in 2016 based on the external dataset. For the
  visualisation, we have used plotly and matplotlib.  
  
  \section{Results Understanding}
  \section{Limitations}
  \section{Context}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
